{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ecea61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8e5c690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "#trained = False\n",
    "trained = True\n",
    "emb_filename = 'images_weights.pickle'\n",
    "num_images = 6\n",
    "\n",
    "# Loading the Trained Model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4c4d34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    \n",
    "    '''Uploading Images'''\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "453d7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 175\n"
     ]
    }
   ],
   "source": [
    "if trained:\n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_emb_tensors = pickle.load(fIn)\n",
    "    print(\"Images:\", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('Data/*.jpg'))\n",
    "    img_emb = []\n",
    "\n",
    "    for image in tqdm(img_names):\n",
    "        img_emb.append(\n",
    "            model(preprocess(pil_loader(image)).unsqueeze(0)).squeeze(0).detach().numpy()\n",
    "        )\n",
    "    img_emb_tensors = torch.tensor(img_emb)\n",
    "\n",
    "    with open(emb_filename, 'wb') as handle:\n",
    "        pickle.dump([img_names, img_emb_tensors], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f0922bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_compressed_index(n_features):\n",
    "    \n",
    "    '''We build a compressed index for a quick search of the nearest neighbors\n",
    "                        in the space of image features'''\n",
    "    \n",
    "    pca = PCA(n_components=min(n_features, 50))\n",
    "    pca.fit(img_emb_tensors)\n",
    "    compressed_features = pca.transform(img_emb_tensors)\n",
    "    dataset = np.float32(compressed_features)\n",
    "\n",
    "    index_compressed = NearestNeighbors(n_neighbors=5, algorithm='auto', metric='euclidean')\n",
    "    index_compressed.fit(dataset)\n",
    "    return [pca, index_compressed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4f55b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_image(img_path, desc):\n",
    "    \n",
    "    '''Displays the image located on the specified path \n",
    "           and adds a name and description to it'''\n",
    "    \n",
    "    plt.imshow(mpimg.imread(img_path))\n",
    "    plt.xlabel(img_path.split('.')[0], fontsize=12)\n",
    "    plt.title(desc, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cf813bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_images(indices, suptitle, num_images=6):\n",
    "    \n",
    "    '''Displays several images that are most similar to the specified image \n",
    "         using the nearest neighbor indexes returned by the index'''\n",
    "    \n",
    "    plt.figure(figsize=(15, 10), facecolor='white')\n",
    "    \n",
    "    plotnumber = 1\n",
    "    for index in indices[0:num_images]:\n",
    "        if plotnumber <= num_images:\n",
    "            ax = plt.subplot(2, 3, plotnumber)\n",
    "            plt.imshow(mpimg.imread(img_names[index]))\n",
    "            plt.xlabel(img_names[index], fontsize=12)\n",
    "            plotnumber += 1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b499216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, factors, concl=False):\n",
    "    \n",
    "    '''Searches for the most similar images for a given image or image path'''\n",
    "    \n",
    "    number = []\n",
    "    \n",
    "    if concl:\n",
    "        if isinstance(query, str):\n",
    "            img_path = query\n",
    "        else:\n",
    "            img_path = img_names[query]\n",
    "\n",
    "        one_img_emb = torch.tensor(model(preprocess(pil_loader(img_path)).unsqueeze(0)).squeeze(0).detach().numpy())\n",
    "        main_image(img_path, '')\n",
    "\n",
    "        compressor, index_compressed = build_compressed_index(factors)\n",
    "        D, I = index_compressed.kneighbors(\n",
    "            np.float32(compressor.transform([one_img_emb.detach().numpy()])), n_neighbors=10)\n",
    "        similar_images(I[0][1:], str(factors))\n",
    "    else:\n",
    "        if isinstance(query, str):\n",
    "            img_path = query\n",
    "        else:\n",
    "            img_path = img_names[query]\n",
    "\n",
    "        one_img_emb = torch.tensor(model(preprocess(pil_loader(img_path)).unsqueeze(0)).squeeze(0).detach().numpy())\n",
    "\n",
    "        compressor, index_compressed = build_compressed_index(factors)\n",
    "        D, I = index_compressed.kneighbors(\n",
    "            np.float32(compressor.transform([one_img_emb.detach().numpy()])), n_neighbors=10)\n",
    "\n",
    "        for i, index in enumerate(I[0][1:]):\n",
    "            if i < factors:\n",
    "                path = img_names[index]\n",
    "                filename = os.path.splitext(os.path.split(path)[1])[0]\n",
    "                number.append(filename.split('.')[0])\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bd3fb725",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['28', '61', '20', '96', '21', '48', '47', '169', '11']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the search function with a model\n",
    "\n",
    "#search(90,300)\n",
    "search(\"1.jpg\", 500, concl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d631a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
